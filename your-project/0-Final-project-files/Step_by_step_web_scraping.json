{"paragraphs":[{"text":"%md\n<style>\n.window {\n  border-radius: 3px;\n  background: #222;\n  color: #fff;\n  overflow: hidden;\n  position: relative;\n  margin: 0 auto;\n  width: 100%;\n  \n}\n\n.terminal {\n  margin: 20px;\n  font-family: monospace;\n  font-size: 10px;\n  color: White;\n  \n  .command { \n    width: 0%;\n    white-space: nowrap;\n    overflow: hidden;\n    animation: write-command 5s both;\n    \n    &:before {\n      content: '$ ';\n      color: #22da26;\n    }\n  }\n  \n  .log {\n    white-space: nowrap;\n    overflow: hidden;\n    animation: write-log 10s both;\n  }\n\n}\n\nbody {\n\n  background-color: black;\n}\n\n.paragraph-margin {\n    margin-right: 5px;\n    margin-left: 5px;\n}\n.new-paragraph {\n\n    height: 13px;\n\n}\n.p_title\n{color: black;\nfont-family: 'Nunito', sans-serif ;\nfont-size: 18px;\nfont-weight: bold;\n}\n.p_title2\n{color: black;\nfont-family: 'Nunito', sans-serif ;\nfont-size: 16px;\nfont-weight: normal;\n}\n.p_subtitle\n{color: black;\nfont-family: 'Nunito', sans-serif ;\nfont-size: 12px ;\nfont-weight: bold;\n}\n.p_text\n{color: black;\nfont-family: 'Nunito', sans-serif ;\nfont-size: 12px;\n}\nol{\nlist-style-type: none;\nmargin: 0;\npadding: 0;\nfont-family: 'Nunito', sans-serif ;\nfont-size: 12px ;\nfont-weight: bold\n}\n\n</style>\n<link href=\"https://fonts.googleapis.com/css?family=Nunito:200&display=swap\" rel=\"stylesheet\">\n<div>\n<p class=\"p_title\">WEB Scraping</p>\n<p class=\"p_subtitle\">Authors: Alberto Abreu, Josep Foradada and Íngrid Munné</p>\n<div class=\"p_text\">This paper is a step-by-step guide carried out in the part of web-scraping.</div>\n<div class=\"p_text\"><p style=\"font-style:italic\">\"Webscraping is a method of data mining from web sites that uses software to extract all the information available from the targeted site by simulating human behavior.\"<a href=\"https://www.linkedin.com/in/ethanjarrell\" class=\"p_subtitle\"> by Ethan Jarrell</a></p></div>\n<ol>\n<li>Libraries</li>\n<li>Translation</li>\n<li>Language detection</li>\n<li>Onlinenewspaperlist</li>\n<li><ul>\n<li>Online</li>\n<li>Offline</li></ul></li>\n<li>GoogleSearch</li>\n<li><ul>\n<li>Online</li>\n<li>Offline</li></ul></li>\n<li>Additional resources</li>\n<li>Data visualization</li>\n</ol>\n<div class=\"p_text\">In this case, the original idea was to use a list of global media and look up all the news that contained the word refugees in the language that are the newspapers written, but with a scalable code for any concept.</div>\n</div> ","user":"anonymous","dateUpdated":"2019-07-20T20:44:09+0200","config":{"colWidth":12,"fontSize":9,"enabled":false,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<style>\n.window {\n  border-radius: 3px;\n  background: #222;\n  color: #fff;\n  overflow: hidden;\n  position: relative;\n  margin: 0 auto;\n  width: 100%;\n  \n}\n\n.terminal {\n  margin: 20px;\n  font-family: monospace;\n  font-size: 10px;\n  color: White;\n  \n  .command { \n    width: 0%;\n    white-space: nowrap;\n    overflow: hidden;\n    animation: write-command 5s both;\n    \n    &:before {\n      content: '$ ';\n      color: #22da26;\n    }\n  }\n  \n  .log {\n    white-space: nowrap;\n    overflow: hidden;\n    animation: write-log 10s both;\n  }\n\n}\n\nbody {\n\n  background-color: black;\n}\n\n.paragraph-margin {\n    margin-right: 5px;\n    margin-left: 5px;\n}\n.new-paragraph {\n\n    height: 13px;\n\n}\n.p_title\n{color: black;\nfont-family: 'Nunito', sans-serif ;\nfont-size: 18px;\nfont-weight: bold;\n}\n.p_title2\n{color: black;\nfont-family: 'Nunito', sans-serif ;\nfont-size: 16px;\nfont-weight: normal;\n}\n.p_subtitle\n{color: black;\nfont-family: 'Nunito', sans-serif ;\nfont-size: 12px ;\nfont-weight: bold;\n}\n.p_text\n{color: black;\nfont-family: 'Nunito', sans-serif ;\nfont-size: 12px;\n}\nol{\nlist-style-type: none;\nmargin: 0;\npadding: 0;\nfont-family: 'Nunito', sans-serif ;\nfont-size: 12px ;\nfont-weight: bold\n}\n\n</style>\n<link href=\"https://fonts.googleapis.com/css?family=Nunito:200&display=swap\" rel=\"stylesheet\">\n<div>\n<p class=\"p_title\">WEB Scraping</p>\n<p class=\"p_subtitle\">Authors: Alberto Abreu, Josep Foradada and Íngrid Munné</p>\n<div class=\"p_text\">This paper is a step-by-step guide carried out in the part of web-scraping.</div>\n<div class=\"p_text\"><p style=\"font-style:italic\">\"Webscraping is a method of data mining from web sites that uses software to extract all the information available from the targeted site by simulating human behavior.\"<a href=\"https://www.linkedin.com/in/ethanjarrell\" class=\"p_subtitle\"> by Ethan Jarrell</a></p></div>\n<ol>\n<li>Libraries</li>\n<li>Translation</li>\n<li>Language detection</li>\n<li>Onlinenewspaperlist</li>\n<li><ul>\n<li>Online</li>\n<li>Offline</li></ul></li>\n<li>GoogleSearch</li>\n<li><ul>\n<li>Online</li>\n<li>Offline</li></ul></li>\n<li>Additional resources</li>\n<li>Data visualization</li>\n</ol>\n<div class=\"p_text\">In this case, the original idea was to use a list of global media and look up all the news that contained the word refugees in the language that are the newspapers written, but with a scalable code for any concept.</div>\n</div>\n</div>"}]},"apps":[],"jobName":"paragraph_1563628862874_-269011248","id":"20190720-152102_1690836791","dateCreated":"2019-07-20T15:21:02+0200","dateStarted":"2019-07-20T20:41:57+0200","dateFinished":"2019-07-20T20:41:57+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:4515"},{"text":"%md\n<div class=\"p_title\">WEB Scraping: <b class=\"p_title2\">Libraries</b></div>\n<div class=\"p_text\">The libraries used along the web scraping.</div>","user":"anonymous","dateUpdated":"2019-07-20T20:32:42+0200","config":{"colWidth":12,"fontSize":9,"enabled":false,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<div class=\"p_title\">WEB Scraping: <b class=\"p_title2\">Libraries</b></div>\n<div class=\"p_text\">The libraries used along the web scraping.</div>\n</div>"}]},"apps":[],"jobName":"paragraph_1563617928394_1262246101","id":"20190720-121848_333313523","dateCreated":"2019-07-20T12:18:48+0200","dateStarted":"2019-07-20T20:32:36+0200","dateFinished":"2019-07-20T20:32:36+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4516"},{"text":"import pandas as pd\nimport os\nimport requests\nimport time # time delay for requests\nimport re   # regex\nfrom requests_html import HTMLSession # to start a HTML session ()\n\n# this library is used to translate any text with the code language\nfrom textblob import TextBlob\n\n# this library is used to translate any text with the code language\nfrom bs4 import BeautifulSoup\n\n# this library is used to detect the input language\nfrom langdetect import detect\n\n","user":"anonymous","dateUpdated":"2019-07-20T20:44:25+0200","config":{"colWidth":12,"fontSize":9,"enabled":false,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1563620796760_328557659","id":"20190720-130636_926618834","dateCreated":"2019-07-20T13:06:36+0200","dateStarted":"2019-07-20T16:07:50+0200","dateFinished":"2019-07-20T16:07:50+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4517"},{"text":"\n%md\n<div class=\"p_title\">WEB Scraping: <b class=\"p_title2\">Translation</b></div>\n<div class=\"p_text\">This will provide a translation for any topic to do later the google inside site search.</div>","user":"anonymous","dateUpdated":"2019-07-20T20:33:18+0200","config":{"colWidth":12,"fontSize":9,"enabled":false,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<div class=\"p_title\">WEB Scraping: <b class=\"p_title2\">Translation</b></div>\n<div class=\"p_text\">This will provide a translation for any topic to do later the google inside site search.</div>\n</div>"}]},"apps":[],"jobName":"paragraph_1563621552415_421929890","id":"20190720-131912_486532862","dateCreated":"2019-07-20T13:19:12+0200","dateStarted":"2019-07-20T20:33:15+0200","dateFinished":"2019-07-20T20:33:15+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4518"},{"text":"# input not working \n# sting_to_analyze = input(\"Wich event you want to analyze: \")\n\nblob = TextBlob(\"refugiados\") # blob = TextBlob(sting_to_analyze)\nprint(\"The translation for the spanish word 'refugiados' is %s\" % (blob.translate(to=\"en\"))) ","user":"anonymous","dateUpdated":"2019-07-20T16:37:03+0200","config":{"colWidth":12,"fontSize":9,"enabled":false,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"The translation for the spanish word 'refugiados' is refugees\n"}]},"apps":[],"jobName":"paragraph_1563622161332_-1227647458","id":"20190720-132921_773480738","dateCreated":"2019-07-20T13:29:21+0200","dateStarted":"2019-07-20T14:51:23+0200","dateFinished":"2019-07-20T14:51:23+0200","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:4519"},{"text":"%md\n<div class=\"p_title\">WEB Scraping: <b class=\"p_title2\">Language detection</b></div>\n<div class=\"p_text\">This will provide a translation for any topic to do later the google inside site search.</div>","user":"anonymous","dateUpdated":"2019-07-20T20:33:31+0200","config":{"colWidth":12,"fontSize":9,"enabled":false,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<div class=\"p_title\">WEB Scraping: <b class=\"p_title2\">Language detection</b></div>\n<div class=\"p_text\">This will provide a translation for any topic to do later the google inside site search.</div>\n</div>"}]},"apps":[],"jobName":"paragraph_1563631514816_570916919","id":"20190720-160514_634855040","dateCreated":"2019-07-20T16:05:14+0200","dateStarted":"2019-07-20T20:33:29+0200","dateFinished":"2019-07-20T20:33:29+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4520"},{"text":"url = 'http://www.botswanaguardian.co.bw'\n\nres = requests.get(url)\nhtml_page = res.content\nsoup = BeautifulSoup(html_page, 'html.parser')\n\ntext = list(soup)\ntext =str(text)\n\ntext = re.sub('<\\s*?script[\\s\\S]*?(/script>)\\W|<[^>]*>','', text) # using regex to remove all tag blocks but first, the <scripts> ones that are multiline.\ntext.rstrip('\\n').rstrip('\\r')\ntext.lstrip('\\n').lstrip('\\r')\n\nprint(\"The language of the website is: %s\" % detect(text))","user":"anonymous","dateUpdated":"2019-07-20T20:44:40+0200","config":{"colWidth":12,"fontSize":9,"enabled":false,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"The language of the website is: en\n"}]},"apps":[],"jobName":"paragraph_1563631507712_-453395541","id":"20190720-160507_1340199247","dateCreated":"2019-07-20T16:05:07+0200","dateStarted":"2019-07-20T16:37:29+0200","dateFinished":"2019-07-20T16:37:34+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4521"},{"text":"\n%md\n<div class=\"p_title\">WEB Scraping: <b class=\"p_title2\">Onlinenespaperlist.com</b></div>\n<p class=\"p_text\"> World online newspapers list is media directory of all Online newspapers television and magazines in the globe. It is number one newspapers and media yellow book in the world</p>","user":"anonymous","dateUpdated":"2019-07-20T20:44:45+0200","config":{"colWidth":12,"fontSize":9,"enabled":false,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<div class=\"p_title\">WEB Scraping: <b class=\"p_title2\">Onlinenespaperlist.com</b></div>\n<p class=\"p_text\"> World online newspapers list is media directory of all Online newspapers television and magazines in the globe. It is number one newspapers and media yellow book in the world</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1563615821578_-206368060","id":"20190720-114341_1799987232","dateCreated":"2019-07-20T11:43:41+0200","dateStarted":"2019-07-20T20:34:12+0200","dateFinished":"2019-07-20T20:34:12+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4522"},{"text":"\n%md\n<div class=\"p_title\">Onlinenespaperlist.com: <b class=\"p_title2\">Online data scraping</b></div>\n<p class=\"p_text\">The code below is done to request the html code from the site. As we will see, it returns an error by the firewall https://modsecurity.org</p>","user":"anonymous","dateUpdated":"2019-07-20T20:44:57+0200","config":{"colWidth":12,"fontSize":9,"enabled":false,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<div class=\"p_title\">Onlinenespaperlist.com: <b class=\"p_title2\">Online data scraping</b></div>\n<p class=\"p_text\">The code below is done to request the html code from the site. As we will see, it returns an error by the firewall https://modsecurity.org</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1563626121148_1347632749","id":"20190720-143521_345482586","dateCreated":"2019-07-20T14:35:21+0200","dateStarted":"2019-07-20T20:34:11+0200","dateFinished":"2019-07-20T20:34:11+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4523"},{"text":"%python\n\nurl = 'https://onlinenewspaperlist.com'\nres = requests.get(url)\nhtml_page = res.content\n\nsoup = BeautifulSoup(html_page, 'lxml')\nprint(soup)\n","user":"anonymous","dateUpdated":"2019-07-20T15:43:44+0200","config":{"colWidth":12,"fontSize":9,"enabled":false,"results":{"0":{"graph":{"mode":"table","height":92.8,"optionOpen":false}}},"editorSetting":{"language":"python","editOnDblClick":false,"completionSupport":true},"editorMode":"ace/mode/python","editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1563626798673_1025090685","id":"20190720-144638_996594864","dateCreated":"2019-07-20T14:46:38+0200","dateStarted":"2019-07-20T14:49:04+0200","dateFinished":"2019-07-20T14:49:06+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4524"},{"text":"%md\n<style>\n\n</style>\n<br>\n<div class=\"window\">\n  <div class=\"terminal\">\n    <p class=\"command\">$ python get_newspapers_of_the_world.py </p>\n    <p class=\"log\"><html><head><title>Not Acceptable!</title></head><body><h1>Not Acceptable!</h1><p>An appropriate representation of the requested resource could not be found on this server. This error was generated by Mod_Security.</p></body></html>\n\n    </p>\n    <p class=\"command\">Process finished with exit code 0</p>\n  </div>\n</div>\n","user":"anonymous","dateUpdated":"2019-07-20T20:03:24+0200","config":{"colWidth":12,"fontSize":9,"enabled":false,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<style>\n\n</style>\n<br>\n<div class=\"window\">\n  <div class=\"terminal\">\n    <p class=\"command\">$ python get_newspapers_of_the_world.py </p>\n    <p class=\"log\"><html><head><title>Not Acceptable!</title></head><body><h1>Not Acceptable!</h1><p>An appropriate representation of the requested resource could not be found on this server. This error was generated by Mod_Security.</p></body></html>\n\n    </p>\n    <p class=\"command\">Process finished with exit code 0</p>\n  </div>\n</div>\n</div>"}]},"apps":[],"jobName":"paragraph_1563626968053_-1715708838","id":"20190720-144928_1853410768","dateCreated":"2019-07-20T14:49:28+0200","dateStarted":"2019-07-20T17:47:21+0200","dateFinished":"2019-07-20T17:47:21+0200","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:4525"},{"text":"%md\n<BR>\n<div class=\"p_text\">After not having access to the html code of the application and not having found another list so complete with the press of the whole world, it is decided to try to download the main page and all its subpages to be able to make a local message. Therefore, we use the HTTracker application to carry it out.  <a href=\"http://www.httrack.com/page/2/en/index.html\" class=\"p_subtitle\">Httrack</a></div>\n","user":"anonymous","dateUpdated":"2019-07-20T20:45:03+0200","config":{"colWidth":12,"fontSize":9,"enabled":false,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<BR>\n<div class=\"p_text\">After not having access to the html code of the application and not having found another list so complete with the press of the whole world, it is decided to try to download the main page and all its subpages to be able to make a local message. Therefore, we use the HTTracker application to carry it out.  <a href=\"http://www.httrack.com/page/2/en/index.html\" class=\"p_subtitle\">Httrack</a></div>\n</div>"}]},"apps":[],"jobName":"paragraph_1563630401648_237769211","id":"20190720-154641_2073113598","dateCreated":"2019-07-20T15:46:41+0200","dateStarted":"2019-07-20T17:58:30+0200","dateFinished":"2019-07-20T17:58:30+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4526"},{"text":"\n%md\n<div class=\"p_title\">Onlinenespaperlist.com: <b class=\"p_title2\">Offline data scraping</b></div>\n<p class=\"p_text\">Once the page is downloaded, it is iterated through the page to capture all the links from \n media that appear on the page for each country. At the same time is applied the language detection for each site</p>","user":"anonymous","dateUpdated":"2019-07-20T20:45:21+0200","config":{"colWidth":12,"fontSize":9,"enabled":false,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<div class=\"p_title\">Onlinenespaperlist.com: <b class=\"p_title2\">Offline data scraping</b></div>\n<p class=\"p_text\">Once the page is downloaded, it is iterated through the page to capture all the links from \n media that appear on the page for each country. At the same time is applied the language detection for each site</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1563633176016_452665903","id":"20190720-163256_2106894566","dateCreated":"2019-07-20T16:32:56+0200","dateStarted":"2019-07-20T20:45:19+0200","dateFinished":"2019-07-20T20:45:19+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4527"},{"text":"%python\n\nwith open('/home/josep/websites/Onlinenewsletter/www.onlinenewspaperlist.com/index.html', 'r') as html_page:\n    html_page = html_page.read()\n\n# the module bs4 (BeautifulSoup) is needed to pars the file to a html website.\n\nsoup = BeautifulSoup(html_page, 'lxml')\n\n# looking fot the tag <div> that has the class 'col_midle col_97' in html <div ... class=\"col_midle col_97\" ...>\n\ndata = soup.findAll('div', {'class': 'col_midle col_97'}) \nlist_countys = []\nlist_countys_webs = []\nfor div in data:\n    \n    # looking fot the <a> tag sibling from <div>\n    # the <a> tag is used to put the links for any website (internals or externals)\n    \n    links = div.findAll('a')\n    for a in links:\n        \n        # inside the a tag we have the link at <a href=\"www.thisisanexample.com\"></a>\n        \n        url = a.get('href')\n        contents = a.text\n        if url.startswith('..') is True:\n            url = url[3:]\n            var = (url, contents)\n            list_countys.append(var)\n\nfor x in list_countys:\n    x = x[0]\n    with open('/home/josep/websites/Onlinenewsletter/' + x, 'r') as html_page2:\n        html_page2 = html_page2.read()\n        html_page2 = str(html_page2)\n        html_page2.encode('utf-8').decode(\"utf-8\")\n\n    soup = BeautifulSoup(html_page2, 'html.parser')\n\n    data = soup.findAll('div', {'class': 'topsitelink_contain clr-fix'})\n\n    for div in data:\n        links = div.findAll('a')\n        for a in links:\n            url = a.get('href')\n            if url.startswith('htt') is True:\n                contents = a.text\n                x = x.replace(\"onlinenewspaperlist.com/\",'' )\n                x = x.replace(\".html\",'' )\n                var = (url,x)\n                list_countys_webs.append(var)\n                print(x)\nmedias_list = list_countys_webs\nprint(list_countys_webs)\nmedias_list = set(list_countys_webs)\n\nprint(medias_list)\n\ndf = pd.DataFrame(medias_list)\n\ndf.to_csv('josep.csv', index=False)\n\n# open a new file where to save all the data once the language is detected\n\njosep_news_txt = open(\"josep_news_txt.txt\", \"a\")\n\ndef get_language(website,country):\n\n    print('here')\n    print(website)\n    url = website\n    \n    # adding several random time sleeps along the code avoids a http error request\n    \n    try:\n        res = requests.get(url, verify=False, timeout=3)\n        print(res)\n        if res.status_code == 200:\n            html_page = res.content\n            soup = BeautifulSoup(html_page, 'html.parser')\n            print(type(soup))\n            text = list(soup)\n            text =str(text)\n            if text == None:\n                return 'NaN'\n            text = re.sub('<\\s*?script[\\s\\S]*?(/script>)\\W|<[^>]*>','', text)\n            text.strip('\\n').strip('\\r')\n            text.rstrip('\\n').rstrip('\\r ')\n            text.lstrip('\\n').lstrip('\\r')\n            val_lang = detect(text)\n            josep_news_txt.write(website + \",\" + country + \",\" + val_lang + \"\\n\")\n            time.sleep(random.randint(0,8))\n            return val_lang\n        else:\n            # add NaN (Not a number) if the request code is different to 200 (accept)\n            time.sleep(random.randint(0,8))\n            return 'NaN'\n    except:\n        time.sleep(random.randint(0,8))\n        return 'NaN' # add NaN (Not a number) if the request is timeout\n\ndf['lg'] = df.apply(lambda row: get_language(row[0],row[1]), axis=1)\nprint(df.head(50))\ndf.to_csv('josep2.csv', index=False)","user":"anonymous","dateUpdated":"2019-07-20T17:19:39+0200","config":{"colWidth":12,"fontSize":9,"enabled":false,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1563633794179_587907234","id":"20190720-164314_1332423295","dateCreated":"2019-07-20T16:43:14+0200","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:4528"},{"text":"\n%md\n<div class=\"p_title\">Google Search <b class=\"p_title2\">Online data scraping</b></div>\n<p class=\"p_text\">\nThe following code is used, through the advanced search options of Google, to return 100 results per page for the news contained in a web page and that contain a certain word. Finally a unique file for a list of news is created with the title, the url, the date and the description.</p>","user":"anonymous","dateUpdated":"2019-07-20T20:45:38+0200","config":{"colWidth":12,"fontSize":9,"enabled":false,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"editorMode":"ace/mode/markdown","tableHide":false,"editorHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<div class=\"p_title\">Google Search <b class=\"p_title2\">Online data scraping</b></div>\n<p class=\"p_text\">\nThe following code is used, through the advanced search options of Google, to return 100 results per page for the news contained in a web page and that contain a certain word. Finally a unique file for a list of news is created with the title, the url, the date and the description.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1563635842860_2023848304","id":"20190720-171722_968913998","dateCreated":"2019-07-20T17:17:22+0200","dateStarted":"2019-07-20T20:45:33+0200","dateFinished":"2019-07-20T20:45:33+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4529"},{"text":"%python\n\njosep_news_articles = open(\"josep_news_articles.txt\", \"a\")\n\ntopic = \"refugees\"\ndate_from_mmddyyyy = \"07/09/2019\"\ndate_to_mmddyyyy = \"07/17/2019\"\n\n\ngoogle_bf = \"https://www.google.com/search?q=site:\"\n\n# parameters for the query\n# the ideal is to include all them as a multiple parameters but understanding the url is a plus https://moz.com/blog/the-ultimate-guide-to-the-google-search-parameters\n\ndate_from = \"&tbs=cdr%3A1%2Ccd_min:\"\ndate_to = \"%2Ccd_max:\"\nnum = '&num=100'\nstart = '&start=0'\nfilter = \"&filter=0\"\n\ntest = ['https://www.lavanguardia.es','','es']\nwebsite = test[0]\nlang = test[2]\nsting_to_analyze = input(\"Wich event you want to analyze: \")\nstring_to_analyze = sting_to_analyze\nblob = TextBlob(string_to_analyze)\ntopic = str(blob.translate(to=lang))\ncontent = '+\"' + topic + '\"'\n\n# here it generates the url for the requests\n\nurl_to_search = google_bf + website + content + date_from + date_from_mmddyyyy + date_to + date_to_mmddyyyy + num + start + filter\nprint(url_to_search)\n\nr = requests.get(url_to_search)\nprint(r.text)\n\n# below, as we did with the onlinenewspaperslist we iterate alongside all the heml code to find what to extract. \n\nif r.status_code == 200:\n    print('HERE1')\n\n    r = str(r.text)\n    r.encode('utf-8').decode(\"utf-8\")\n    soup = BeautifulSoup(r, 'html.parser')\n\n    print('HERE2')\n    data = soup.findAll('div', {'class': 'ZINbbc xpd O9g5cc uUPGi'}) # get all divs with news individually\n    if soup.findAll('div', {'class': 'nMymef MUxGbd lyLwlc'}):  # find if there is a next button to iterate through multiple google searchresult pages if there are. (This functions was )\n        for div in data:\n            print('HERE3')\n            find_data2 = div.find('div', {'class': 'jfp3ef'})\n            find_data3 = div.find('div', {'class': 'BNeawe s3v9rd AP7Wnd'})\n            if find_data2:\n                for div2 in find_data2:\n                    print('HERE4')\n                    print(div2)\n                    links = div.findAll('a')\n                    for a in links:\n                        print('HER54')\n                        url = str(a.get('href')) # get the url of the new\n                        num = url.find('&')\n                        url = url[7:num]\n                    titles = div.findAll('div', {'class': 'ZINbbc xpd O9g5cc uUPGi'})\n                    for a2 in links:\n                        print('HER64')\n                        title = str(a2.contents[0]) # get the content of the div selected. In this case the title of the new\n                        print(title)\n                        title = title[34:-6]\n                        title = title.replace(',', '')\n            if find_data3:\n                for div4 in find_data3:\n                    data4 = div4.findAll('div', {'class': 'BNeawe s3v9rd AP7Wnd'})\n\n                    for div5 in data4:\n                            print('HER74')\n                            if div5.contents[0]:\n                                get_data = str(div5.contents[0])\n                                get_data = get_data[28:-7]\n                                get_data = get_data.replace(',', '')\n                                get_data = get_data.replace('.', '')\n                                print(get_data)\n                            if len(div5.contents) >= 2:\n                                new_description = str(div5.contents[2])\n                                new_description = new_description.replace(',', '')\n                                print(new_description)\n            if title and url and get_data and new_description: # error handling: if some of the divs don't have the data, avoid writting the website into a file.\n                josep_news_articles.write( title +  \",\" + url + \",\" + get_data + \",\" + new_description + \"\\n\")\n\nelse:\n    print('NaN')","user":"anonymous","dateUpdated":"2019-07-20T19:13:11+0200","config":{"colWidth":12,"fontSize":9,"enabled":false,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1563636152987_1063551464","id":"20190720-172232_2086585265","dateCreated":"2019-07-20T17:22:32+0200","dateStarted":"2019-07-20T19:12:09+0200","dateFinished":"2019-07-20T19:12:31+0200","status":"ABORT","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4530"},{"text":"%md<p>\n<p class=\"p_text\">But when itering this for all the websites we captured from onlinenewspaperlist.com this happens:</p>","user":"anonymous","dateUpdated":"2019-07-20T19:13:53+0200","config":{"colWidth":12,"fontSize":9,"enabled":false,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>\n<p class=\"p_text\">But when itering this for all the websites we captured from onlinenewspaperlist.com this happens:</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1563636965010_1882530622","id":"20190720-173605_2116743327","dateCreated":"2019-07-20T17:36:05+0200","dateStarted":"2019-07-20T19:13:48+0200","dateFinished":"2019-07-20T19:13:48+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4531"},{"text":"%md\n<style>\n\n</style>\n<br>\n<div class=\"window\">\n  <div class=\"terminal\">\n    <p class=\"command\">$ python get_all_news_from_google.py.py </p>\n    <p class=\"log\"><!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\">\n<html>\n<head></head>\n<body>\n<div style=\"max-width:400px;\">\n<hr noshade size=\"1\" style=\"color:#ccc; background-color:#ccc;\"><br>\n<form id=\"captcha-form\" action=\"index\" method=\"post\">\n<script src=\"https://www.google.com/recaptcha/api.js\" async defer></script>\n<script>var submitCallback = function(response) {document.getElementById('captcha-form').submit();};</script>\n<div id=\"recaptcha\" class=\"g-recaptcha\" data-sitekey=\"6LfwuyUTAAAAAOAmoS0fdqijC2PbbdH4kjq62Y1b\" data-callback=\"submitCallback\" data-s=\"jUqnKOee1TVvUchLT4-wetb4yBGffgvNX_y-GHNcC6W4jZ9qSM2D2jGvCBVlDo7U_8rjwW4LFaIAI1mS1au-MD5V6cYWisAA-bLaiQyV_Uji7gYcFdN1yfvRgjzjSe_BQf7cJM9Pwu8rc7Uu4I65UsWj_vRTZq8KH3rgHGPJnQF0gNp3SsX2-sxOHBYOAQdWoLVtbFFLsP2IaR_AFaTpky2QkqY0RAvL0QeMK5O82XkI5f3CCYHMv8Y\"></div>\n<input type='hidden' name='q' value='EgQCiuHMGPzrzOkFIhkA8aeDS2lSF3BJJktF8fdJJKhqtbSuPkxOMgFy'><input type=\"hidden\" name=\"continue\" value=\"https://www.google.com/search?q=site:http://www.kansanuutiset.fi/+%22pakolaisten%22&amp;tbs=qdr:w&amp;num=100&amp;start=0&amp;filter=0\">\n</form>\n<hr noshade size=\"1\" style=\"color:#ccc; background-color:#ccc;\">\n\n<div style=\"\">\n<b>About this page</b><br><br>Our systems have detected unusual traffic from your computer network.  This page checks to see if it&#39;s really you sending the requests, and not a robot. \n<div id=\"infoDiv\" style=\"\">\nThis page appears when Google automatically detects requests coming from your computer network which appear to be in violation of the <a href=\"//www.google.com/policies/terms/\">Terms of Service</a>. The block will expire shortly after those requests stop.  In the meantime, solving the above CAPTCHA will let you continue to use our services.<br><br>This traffic may have been sent by malicious software, a browser plug-in, or a script that sends automated requests.  If you share your network connection, ask your administrator for help &mdash; a different computer using the same IP address may be responsible.  <a href=\"//support.google.com/websearch/answer/86640\">Learn more</a><br><br>Sometimes you may be asked to solve the CAPTCHA if you are using advanced terms that robots are known to use, or sending requests very quickly.\n</div>\n<br>\nIP address: 2.138.225.204<br>Time: 2019-07-20T15:40:46Z<br>URL: https://www.google.com/search?q=site:http://www.kansanuutiset.fi/+%22pakolaisten%22&amp;tbs=qdr:w&amp;num=100&amp;start=0&amp;filter=0<br>\n</div>\n</div>\n</body>\n</html>\n\n\n    </p>\n    <p class=\"command\">Process finished with exit code 0</p>\n  </div>\n</div>\n","user":"anonymous","dateUpdated":"2019-07-20T20:45:59+0200","config":{"colWidth":12,"fontSize":9,"enabled":false,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<style>\n\n</style>\n<br>\n<div class=\"window\">\n  <div class=\"terminal\">\n    <p class=\"command\">$ python get_all_news_from_google.py.py </p>\n    <p class=\"log\"><!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\">\n<html>\n<head></head>\n<body>\n<div style=\"max-width:400px;\">\n<hr noshade size=\"1\" style=\"color:#ccc; background-color:#ccc;\"><br>\n<form id=\"captcha-form\" action=\"index\" method=\"post\">\n<script src=\"https://www.google.com/recaptcha/api.js\" async defer></script>\n<script>var submitCallback = function(response) {document.getElementById('captcha-form').submit();};</script>\n<div id=\"recaptcha\" class=\"g-recaptcha\" data-sitekey=\"6LfwuyUTAAAAAOAmoS0fdqijC2PbbdH4kjq62Y1b\" data-callback=\"submitCallback\" data-s=\"jUqnKOee1TVvUchLT4-wetb4yBGffgvNX_y-GHNcC6W4jZ9qSM2D2jGvCBVlDo7U_8rjwW4LFaIAI1mS1au-MD5V6cYWisAA-bLaiQyV_Uji7gYcFdN1yfvRgjzjSe_BQf7cJM9Pwu8rc7Uu4I65UsWj_vRTZq8KH3rgHGPJnQF0gNp3SsX2-sxOHBYOAQdWoLVtbFFLsP2IaR_AFaTpky2QkqY0RAvL0QeMK5O82XkI5f3CCYHMv8Y\"></div>\n<input type='hidden' name='q' value='EgQCiuHMGPzrzOkFIhkA8aeDS2lSF3BJJktF8fdJJKhqtbSuPkxOMgFy'><input type=\"hidden\" name=\"continue\" value=\"https://www.google.com/search?q=site:http://www.kansanuutiset.fi/+%22pakolaisten%22&amp;tbs=qdr:w&amp;num=100&amp;start=0&amp;filter=0\">\n</form>\n<hr noshade size=\"1\" style=\"color:#ccc; background-color:#ccc;\">\n\n<div style=\"\">\n<b>About this page</b><br><br>Our systems have detected unusual traffic from your computer network.  This page checks to see if it&#39;s really you sending the requests, and not a robot. \n<div id=\"infoDiv\" style=\"\">\nThis page appears when Google automatically detects requests coming from your computer network which appear to be in violation of the <a href=\"//www.google.com/policies/terms/\">Terms of Service</a>. The block will expire shortly after those requests stop.  In the meantime, solving the above CAPTCHA will let you continue to use our services.<br><br>This traffic may have been sent by malicious software, a browser plug-in, or a script that sends automated requests.  If you share your network connection, ask your administrator for help &mdash; a different computer using the same IP address may be responsible.  <a href=\"//support.google.com/websearch/answer/86640\">Learn more</a><br><br>Sometimes you may be asked to solve the CAPTCHA if you are using advanced terms that robots are known to use, or sending requests very quickly.\n</div>\n<br>\nIP address: 2.138.225.204<br>Time: 2019-07-20T15:40:46Z<br>URL: https://www.google.com/search?q=site:http://www.kansanuutiset.fi/+%22pakolaisten%22&amp;tbs=qdr:w&amp;num=100&amp;start=0&amp;filter=0<br>\n</div>\n</div>\n</body>\n</html>\n\n\n    </p>\n    <p class=\"command\">Process finished with exit code 0</p>\n  </div>\n</div>\n</div>"}]},"apps":[],"jobName":"paragraph_1563637533879_-1943100223","id":"20190720-174533_393366845","dateCreated":"2019-07-20T17:45:33+0200","dateStarted":"2019-07-20T20:01:09+0200","dateFinished":"2019-07-20T20:01:09+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4532"},{"text":"%md<p>\n<p class=\"p_text\">Seeing that we have not been able to obtain all the results with the robot and that those that we received by the request did not respect the search by date (because it did not have an API key), we decided to work locally and to download the response from the Google Serch website with the media search and keyword.</p></p>","user":"anonymous","dateUpdated":"2019-07-20T20:46:09+0200","config":{"colWidth":12,"fontSize":9,"enabled":false,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>\n<p class=\"p_text\">Seeing that we have not been able to obtain all the results with the robot and that those that we received by the request did not respect the search by date (because it did not have an API key), we decided to work locally and to download the response from the Google Serch website with the media search and keyword.</p></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1563642824233_1646196857","id":"20190720-191344_1356588781","dateCreated":"2019-07-20T19:13:44+0200","dateStarted":"2019-07-20T19:56:11+0200","dateFinished":"2019-07-20T19:56:11+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4533"},{"text":"%md<p>\n<p class=\"p_text\">But the downloaded file for the search page using Firefox was aother obstacle, html files contained special characters, such as spaces, dots or double quotes. This caused that the file could not be read with python because the double quotes. Therefore, we had to transform all the names of the folders using terminal and regex (PCRE - Perl Regular Expressions)</p></p>\n<div class=\"window\">\n  <div class=\"terminal\">\n    <p class=\"command\">$ rename -n 's/:|\"| |-|_//g' *</p>\n  </div>\n</div>","user":"anonymous","dateUpdated":"2019-07-20T20:46:15+0200","config":{"colWidth":12,"fontSize":9,"enabled":false,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>\n<p class=\"p_text\">But the downloaded file for the search page using Firefox was aother obstacle, html files contained special characters, such as spaces, dots or double quotes. This caused that the file could not be read with python because the double quotes. Therefore, we had to transform all the names of the folders using terminal and regex (PCRE - Perl Regular Expressions)</p></p>\n<div class=\"window\">\n  <div class=\"terminal\">\n    <p class=\"command\">$ rename -n 's/:|\"| |-|_//g' *</p>\n  </div>\n</div>\n</div>"}]},"apps":[],"jobName":"paragraph_1563644293343_1966653486","id":"20190720-193813_1985180294","dateCreated":"2019-07-20T19:38:13+0200","dateStarted":"2019-07-20T20:07:53+0200","dateFinished":"2019-07-20T20:07:53+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4534"},{"text":"%md\n<div class=\"p_title\">Google Search <b class=\"p_title2\">Offline data scraping</b></div>\n<p class=\"p_text\">\nOnce we had all the websites ready to be scraped, some variations to the previous code made the trick. We also saw that the downloaded website had different classes of requests through Python and therefore had to redo the code</p>","user":"anonymous","dateUpdated":"2019-07-20T20:46:20+0200","config":{"colWidth":12,"fontSize":9,"enabled":false,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<div class=\"p_title\">Google Search <b class=\"p_title2\">Offline data scraping</b></div>\n<p class=\"p_text\">\nOnce we had all the websites ready to be scraped, some variations to the previous code made the trick. We also saw that the downloaded website had different classes of requests through Python and therefore had to redo the code</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1563645869284_900382196","id":"20190720-200429_690404131","dateCreated":"2019-07-20T20:04:29+0200","dateStarted":"2019-07-20T20:29:28+0200","dateFinished":"2019-07-20T20:29:28+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4535"},{"text":"%python\n\n\njosep_news_txt = open(\"josep_news_uk.csv\", \"a\")\njosep_news_txt.write(\"country,url\\n\") # first we add to the text the columns\n\n# iteration between all the file s in the folder\n\nfor filename in os.listdir(\"/home/josep/Documents/website/uk/\"):\n    filename = '/home/josep/Documents/website/uk/' + filename\n    with open(filename, 'r') as html_page:\n        html_page = html_page.read()\n\n    # in this case we just get the url and add the country when saving it\n\n    print('HERE1')\n\n    #r = str(r.text)\n    #r.encode('utf-8').decode(\"utf-8\")\n    soup = BeautifulSoup(html_page, 'html.parser')\n\n    print('HERE2')\n    data = soup.findAll('div', {'class': 'rc'})\n    if soup.findAll('div', {'class': 'rc'}):  \n        for div in data:\n            print(div)\n            title = str(div.contents[0])\n            url = div.get('href')\n            print(title)\n            print(url)\n            data2 = div.find('a', {'class': ''})\n            if data2:\n                print(data2)\n                url = data2.get('href')\n                print(url)\n                josep_news_txt.write(\"uk,\" + url + \"\\n\")\n\n\n","user":"anonymous","dateUpdated":"2019-07-20T20:46:28+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1563647034282_-2025838387","id":"20190720-202354_313549687","dateCreated":"2019-07-20T20:23:54+0200","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:4537"},{"text":"%md\n<div class=\"p_title\">WEB Scraping: <b class=\"p_title2\">Data visualization</b></div>\n<div class=\"p_text\">All the following steps have the objective of addressing all the information obtained through the web scraping and obtaining a visual representation for the ratio of all the news referring to refugees per inhabitant.</div>","user":"anonymous","dateUpdated":"2019-07-20T20:23:49+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<div class=\"p_title\">WEB Scraping: <b class=\"p_title2\">Data visualization</b></div>\n<div class=\"p_text\">All the following steps have the objective of addressing all the information obtained through the web scraping and obtaining a visual representation for the ratio of all the news referring to refugees per inhabitant.</div>\n</div>"}]},"apps":[],"jobName":"paragraph_1563618498455_-1128581015","id":"20190720-122818_1056729438","dateCreated":"2019-07-20T12:28:18+0200","dateStarted":"2019-07-20T20:23:49+0200","dateFinished":"2019-07-20T20:23:49+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4538"},{"text":"\nos.chdir('/home/josep/Documents/DA/GIT/003_Project_Data-Thieves/your-project/josep_test')\nUSA = pd.read_csv(\"josep_news_usa.csv\")\n\nFR = pd.read_csv(\"josep_news_fr.csv\")\nTK = pd.read_csv(\"josep_news_tk.csv\")\nES = pd.read_csv(\"josep_news_es.csv\")\nUK = pd.read_csv(\"josep_news_uk.csv\")\nCA = pd.read_csv(\"josep_news_ca.csv\")\nCountries = pd.read_csv(\"Countries.csv\")\n","user":"anonymous","dateUpdated":"2019-07-20T20:46:31+0200","config":{"colWidth":12,"fontSize":9,"enabled":false,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionSupport":true},"editorMode":"ace/mode/python","editorHide":false,"tableHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1563436432475_-811647749","id":"20190718-095352_1102620462","dateCreated":"2019-07-18T09:53:52+0200","dateStarted":"2019-07-20T17:18:21+0200","dateFinished":"2019-07-20T17:18:21+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4540"},{"text":"print(Countries)","user":"anonymous","dateUpdated":"2019-07-20T20:46:33+0200","config":{"colWidth":12,"fontSize":9,"enabled":false,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"  country  population  np_analyzed\n0     usa   327200000           19\n1   spain    46720000           23\n2  turkey    79810000            6\n3      uk    66040000           17\n4  france    66990000           16\n5  canada    37060000           20\n"}]},"apps":[],"jobName":"paragraph_1563462247122_351498882","id":"20190718-170407_1891314679","dateCreated":"2019-07-18T17:04:07+0200","dateStarted":"2019-07-20T15:43:45+0200","dateFinished":"2019-07-20T15:43:45+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4541"},{"text":"print(USA.head(2))","user":"anonymous","dateUpdated":"2019-07-20T20:46:34+0200","config":{"colWidth":6,"fontSize":9,"enabled":false,"results":{"0":{"graph":{"mode":"table","height":324,"optionOpen":false}}},"editorSetting":{"language":"python","editOnDblClick":false,"completionSupport":true},"editorMode":"ace/mode/python","editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"  country                                                url\n0     usa  https://www.seattletimes.com/nation-world/nati...\n1     usa  https://www.seattletimes.com/entertainment/jef...\n"}]},"apps":[],"jobName":"paragraph_1563459154437_-322664648","id":"20190718-161234_1473524754","dateCreated":"2019-07-18T16:12:34+0200","dateStarted":"2019-07-20T15:43:45+0200","dateFinished":"2019-07-20T15:43:45+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4542"},{"text":"print(CA.head(2))","user":"anonymous","dateUpdated":"2019-07-20T20:46:35+0200","config":{"colWidth":6,"fontSize":9,"enabled":false,"results":{"0":{"graph":{"mode":"table","height":332,"optionOpen":false}}},"editorSetting":{"language":"python","editOnDblClick":false,"completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"  country                                                url\n0  canada  https://www.journaldemontreal.com/2019/06/19/r...\n1  canada  https://www.journaldemontreal.com/2019/07/15/i...\n"}]},"apps":[],"jobName":"paragraph_1563472839836_-1533800101","id":"20190718-200039_1043505921","dateCreated":"2019-07-18T20:00:39+0200","dateStarted":"2019-07-20T15:43:45+0200","dateFinished":"2019-07-20T15:43:45+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4543"},{"text":"print(ES.head(2))","user":"anonymous","dateUpdated":"2019-07-20T20:46:36+0200","config":{"colWidth":6,"fontSize":9,"enabled":false,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"  country                                                url\n0   spain  http://www.telemadrid.es/noticias/internaciona...\n1   spain  http://www.telemadrid.es/programas/telenoticia...\n"}]},"apps":[],"jobName":"paragraph_1563472836648_-740287040","id":"20190718-200036_1065534208","dateCreated":"2019-07-18T20:00:36+0200","dateStarted":"2019-07-20T15:43:45+0200","dateFinished":"2019-07-20T15:43:45+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4544"},{"text":"print(FR.head(2))","user":"anonymous","dateUpdated":"2019-07-20T20:46:37+0200","config":{"colWidth":6,"fontSize":9,"enabled":false,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"  country                                                url\n0  france  https://www.lexpress.fr/actualite/monde/plus-d...\n1  france  https://www.lexpress.fr/actualites/1/styles/il...\n"}]},"apps":[],"jobName":"paragraph_1563472894568_2055218894","id":"20190718-200134_115635219","dateCreated":"2019-07-18T20:01:34+0200","dateStarted":"2019-07-20T15:43:45+0200","dateFinished":"2019-07-20T15:43:45+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4545"},{"text":"print(TK.head(2))","user":"anonymous","dateUpdated":"2019-07-20T20:46:39+0200","config":{"colWidth":6,"fontSize":9,"enabled":false,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"  country                                                url\n0  turkey       http://www.hurriyet.com.tr/haberleri/multeci\n1  turkey  http://www.hurriyet.com.tr/gundem/dunya-multec...\n"}]},"apps":[],"jobName":"paragraph_1563472898800_-1691058950","id":"20190718-200138_794041764","dateCreated":"2019-07-18T20:01:38+0200","dateStarted":"2019-07-20T15:43:46+0200","dateFinished":"2019-07-20T15:43:46+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4546"},{"text":"print(UK.head(2))","user":"anonymous","dateUpdated":"2019-07-20T20:46:40+0200","config":{"colWidth":6,"fontSize":9,"enabled":false,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"  country                                                url\n0      uk  https://www.oxfordtimes.co.uk/news/17712427.co...\n1      uk  https://www.oxfordtimes.co.uk/news/17768446.ex...\n"}]},"apps":[],"jobName":"paragraph_1563472901080_1683340512","id":"20190718-200141_738293905","dateCreated":"2019-07-18T20:01:41+0200","dateStarted":"2019-07-20T15:43:46+0200","dateFinished":"2019-07-20T15:43:46+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4547"},{"text":"#Countries['total_news'] = \n\n#TK.columns\nCountries['total_news'] = 0\nCountries.loc[0,'total_news'] = USA[' url'].shape[0]\nCountries.loc[1,'total_news'] = ES[' url'].shape[0]\nCountries.loc[2,'total_news'] = TK[' url'].shape[0]\nCountries.loc[3,'total_news'] = UK[' url'].shape[0]\nCountries.loc[4,'total_news'] = FR[' url'].shape[0]\nCountries.loc[5,'total_news'] = CA[' url'].shape[0]\n\nCountries.head()","user":"anonymous","dateUpdated":"2019-07-20T20:46:41+0200","config":{"colWidth":12,"fontSize":9,"enabled":false,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"  country  population  np_analyzed  total_news\n0     usa   327200000           19         849\n1   spain    46720000           23        2093\n2  turkey    79810000            6         314\n3      uk    66040000           17         238\n4  france    66990000           16         459\n"}]},"apps":[],"jobName":"paragraph_1563461271700_1690045740","id":"20190718-164751_2024787383","dateCreated":"2019-07-18T16:47:51+0200","dateStarted":"2019-07-20T15:43:46+0200","dateFinished":"2019-07-20T15:43:46+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4548"},{"text":"Countries.info()\nCountries['population'].astype(float)","user":"anonymous","dateUpdated":"2019-07-20T20:46:43+0200","config":{"colWidth":12,"fontSize":9,"enabled":false,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 6 entries, 0 to 5\nData columns (total 4 columns):\ncountry        6 non-null object\npopulation     6 non-null int64\nnp_analyzed    6 non-null int64\ntotal_news     6 non-null int64\ndtypes: int64(3), object(1)\nmemory usage: 272.0+ bytes\n0    327200000.0\n1     46720000.0\n2     79810000.0\n3     66040000.0\n4     66990000.0\n5     37060000.0\nName: population, dtype: float64\n"}]},"apps":[],"jobName":"paragraph_1563474511543_-110813380","id":"20190718-202831_1299667489","dateCreated":"2019-07-18T20:28:31+0200","dateStarted":"2019-07-20T15:43:46+0200","dateFinished":"2019-07-20T15:43:46+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4549"},{"text":"Countries['population'] = Countries['population'].apply(lambda x: int(x)/1000000)\n","user":"anonymous","dateUpdated":"2019-07-20T20:46:44+0200","config":{"colWidth":12,"fontSize":9,"enabled":false,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1563474255005_1685655465","id":"20190718-202415_35100006","dateCreated":"2019-07-18T20:24:15+0200","dateStarted":"2019-07-20T15:43:46+0200","dateFinished":"2019-07-20T15:43:46+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4550"},{"text":"Countries['ratio_per_M'] = (Countries['total_news']/Countries['np_analyzed'])/Countries['population']\n\nCountries.to_csv('news_ratio_country.csv')","user":"anonymous","dateUpdated":"2019-07-20T20:46:46+0200","config":{"colWidth":12,"fontSize":9,"enabled":false,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1563474735403_1970218848","id":"20190718-203215_851840483","dateCreated":"2019-07-18T20:32:15+0200","dateStarted":"2019-07-20T15:43:46+0200","dateFinished":"2019-07-20T15:43:46+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4551"},{"text":"%sql\n\nSELECT * FROM Countries ORDER BY ratio_per_M DESC","user":"anonymous","dateUpdated":"2019-07-20T20:46:47+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{"0":{"graph":{"mode":"multiBarChart","height":674,"optionOpen":false,"setting":{"multiBarChart":{"rotate":{"degree":"-45"},"xLabelStatus":"default","stacked":false},"scatterChart":{"yAxis":{"name":"population","index":1,"aggr":"sum"},"group":{"name":"np_analyzed","index":2,"aggr":"sum"},"xAxis":{"name":"country","index":0,"aggr":"sum"}},"lineChart":{"rotate":{"degree":"-45"},"xLabelStatus":"default"},"pieChart":{},"stackedAreaChart":{"rotate":{"degree":"-45"},"xLabelStatus":"default"}},"keys":[{"name":"country","index":0,"aggr":"sum"}],"groups":[],"values":[{"name":"ratio_per_M","index":4,"aggr":"sum"}],"commonSetting":{}},"helium":{}}},"editorSetting":{"language":"sql","editOnDblClick":false,"completionSupport":false,"completionKey":"TAB"},"editorMode":"ace/mode/sql"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"country\tpopulation\tnp_analyzed\ttotal_news\tratio_per_M\nspain\t46.72\t23\t2093\t1.9477739726027399\ncanada\t37.06\t20\t685\t0.9241770102536427\nturkey\t79.81\t6\t314\t0.6557240111932506\nfrance\t66.99\t16\t459\t0.4282355575459024\nuk\t66.04\t17\t238\t0.21199273167777102\nusa\t327.2\t19\t849\t0.13656543559387466\n"}]},"apps":[],"jobName":"paragraph_1563436450852_-738802042","id":"20190718-095410_2137928926","dateCreated":"2019-07-18T09:54:10+0200","dateStarted":"2019-07-20T15:14:13+0200","dateFinished":"2019-07-20T15:14:13+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4552"},{"text":"%md\n<div class=\"p_text\">With this data now we can join the graph with the data geted through twitter</div>","user":"anonymous","dateUpdated":"2019-07-20T20:46:53+0200","config":{"colWidth":12,"fontSize":9,"enabled":false,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<div class=\"p_text\">With this data now we can join the graph with the data geted through twitter</div>\n</div>"}]},"apps":[],"jobName":"paragraph_1563436480723_-537907929","id":"20190718-095440_1051220040","dateCreated":"2019-07-18T09:54:40+0200","dateStarted":"2019-07-20T20:19:06+0200","dateFinished":"2019-07-20T20:19:06+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4553"}],"name":"Step by step web scraping","id":"2EJCRY438","noteParams":{},"noteForms":{},"angularObjects":{"Python:shared_process":[],"md:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}